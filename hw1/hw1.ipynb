{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 \n",
    "**Describe a machine learning problem that you would like to solve using Logistic Regression. Clearly state why Logistic regression is the best choice for solving this problem.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of predicting credit defaults is an are where logistic regression can thrive. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "**Pick a publicly available dataset (except The Titanic Dataset because we already discussed that in class) that you will use to solve this problem. You must provide a link to the dataset and perform necessary Exploratory Data Analysis (EDA). Clearly demonstrate the steps you follow for your EDA with a justification of why these were required. For example, if the dataset has lot of missing values, then why did you use a specific technique when handling missing data. This task may include data visualization (Check this link : https://www.geeksforgeeks.org/top-8-python-libraries-for-data-visualization/).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "folder_name = \"./credit-card-defaulter-prediction\"\n",
    "file_name = \"Credit Card Defaulter Prediction.csv\"\n",
    "\n",
    "\n",
    "if not os.path.exists(folder_name):\n",
    "    import opendatasets as od\n",
    "    # download the data\n",
    "    dataset_link = \"https://www.kaggle.com/datasets/gauravtopre/credit-card-defaulter-prediction\"\n",
    "    od.download(dataset_link)\n",
    "\n",
    "data = pd.read_csv(os.path.join(folder_name, file_name))\n",
    "# data = data.drop(0)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# randomly separate testing / training data\n",
    "\n",
    "\n",
    "labels = [ ]\n",
    "\n",
    "# one hot encode the categorical variables\n",
    "for label in labels:\n",
    "    data[label] = LabelBinarizer().fit_transform(data[label])\n",
    "\n",
    "# center the data around zero\n",
    "\n",
    "# ensure a variance of 1 for each variable and save offset / multiplier for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "**Implement, Logistic Regression in this step. Clearly write your cost function and derivatives before implementing gradient descent. Do not use any built-in packages for this step. You can use the vectorization techniques demonstrated in class. Implement any 2 variants of gradient descent in their original form. (Refer to the research paper discussed in class).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel:\n",
    "    def __init__(self, cost_fn, input_features):\n",
    "        self.cost_fn = cost_fn\n",
    "        self.weights = np.zeros((input_features, 1))\n",
    "        self.bias = np.zeros((1,))\n",
    "\n",
    "    def _calculate_gradient(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, D):\n",
    "        pass\n",
    "\n",
    "    def gradient_update(self, learning_rate):\n",
    "        pass\n",
    "\n",
    "    def loss(self):\n",
    "        pass\n",
    "    \n",
    "def fit_sgd(dataset_train, dataset_test):\n",
    "    model = LogisticRegressionModel()\n",
    "\n",
    "\n",
    "def fit_batched(dataset_train, dataset_test):\n",
    "    model = LogisticRegressionModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "**Pick any 2 optimization algorithms that are used to optimize the ‘vanilla’ gradient descent. Implement both. You may implement these algorithms yourself OR use a package. In your conclusion, compare both optimization techniques/algorithms with respect to the results you achieve. Also \n",
    "compare these results with the original implementation of gradient descent (Task 3 above). Describe why or why not should we use optimization algorithms for the task at hand.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opendatasets\n",
      "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: tqdm in /Users/danielkopp/opt/anaconda3/envs/tensorflow-base/lib/python3.7/site-packages (from opendatasets) (4.64.0)\n",
      "Collecting kaggle\n",
      "  Using cached kaggle-1.5.12.tar.gz (58 kB)\n",
      "Requirement already satisfied: click in /Users/danielkopp/opt/anaconda3/envs/tensorflow-base/lib/python3.7/site-packages (from opendatasets) (8.0.3)\n",
      "Requirement already satisfied: importlib-metadata in /Users/danielkopp/opt/anaconda3/envs/tensorflow-base/lib/python3.7/site-packages (from click->opendatasets) (4.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /Users/danielkopp/opt/anaconda3/envs/tensorflow-base/lib/python3.7/site-packages (from importlib-metadata->click->opendatasets) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/danielkopp/opt/anaconda3/envs/tensorflow-base/lib/python3.7/site-packages (from importlib-metadata->click->opendatasets) (3.6.0)\n",
      "Requirement already satisfied: six>=1.10 in /Users/danielkopp/opt/anaconda3/envs/tensorflow-base/lib/python3.7/site-packages (from kaggle->opendatasets) (1.15.0)\n",
      "Requirement already satisfied: certifi in /Users/danielkopp/opt/anaconda3/envs/tensorflow-base/lib/python3.7/site-packages (from kaggle->opendatasets) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil in /Users/danielkopp/opt/anaconda3/envs/tensorflow-base/lib/python3.7/site-packages (from kaggle->opendatasets) (2.8.2)\n",
      "Requirement already satisfied: requests in /Users/danielkopp/opt/anaconda3/envs/tensorflow-base/lib/python3.7/site-packages (from kaggle->opendatasets) (2.26.0)\n",
      "Collecting python-slugify\n",
      "  Downloading python_slugify-6.1.2-py2.py3-none-any.whl (9.4 kB)\n",
      "Requirement already satisfied: urllib3 in /Users/danielkopp/opt/anaconda3/envs/tensorflow-base/lib/python3.7/site-packages (from kaggle->opendatasets) (1.26.7)\n",
      "Collecting text-unidecode>=1.3\n",
      "  Using cached text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/danielkopp/opt/anaconda3/envs/tensorflow-base/lib/python3.7/site-packages (from requests->kaggle->opendatasets) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/danielkopp/opt/anaconda3/envs/tensorflow-base/lib/python3.7/site-packages (from requests->kaggle->opendatasets) (2.0.7)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=41a027994178b355cb0649588e247770d39d859ff7e82a91e06968eb581490f6\n",
      "  Stored in directory: /Users/danielkopp/Library/Caches/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n",
      "Successfully built kaggle\n",
      "Installing collected packages: text-unidecode, python-slugify, kaggle, opendatasets\n",
      "Successfully installed kaggle-1.5.12 opendatasets-0.1.22 python-slugify-6.1.2 text-unidecode-1.3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "def get_model(dataset, optimizer):\n",
    "    model = tf.keras.Sequential([\n",
    "        Input((dataset.shape)),\n",
    "        Dense((1), activation=sigmoid)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('tensorflow-base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "94d327540760614ea8be76d670c5f95b4d5227b99a4614ae45cbc07894e729a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

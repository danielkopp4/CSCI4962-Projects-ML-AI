{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 \n",
    "**Describe a machine learning problem that you would like to solve using Logistic Regression. Clearly state why Logistic regression is the best choice for solving this problem.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of predicting credit defaults is an are where logistic regression can thrive. We must be able to predict the likelihood of an individual to default on their credit card. There are many variables to calculate the credit default probability. Thus, logistic regression would be the ideal choice to predict this probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "**Pick a publicly available dataset (except The Titanic Dataset because we already discussed that in class) that you will use to solve this problem. You must provide a link to the dataset and perform necessary Exploratory Data Analysis (EDA). Clearly demonstrate the steps you follow for your EDA with a justification of why these were required. For example, if the dataset has lot of missing values, then why did you use a specific technique when handling missing data. This task may include data visualization (Check this link : https://www.geeksforgeeks.org/top-8-python-libraries-for-data-visualization/).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that is used is from this link: https://www.kaggle.com/datasets/gauravtopre/credit-card-defaulter-prediction.\n",
    "\n",
    "Before I preprocessed the data, I removed the `ID` column as it is not relevant to the data. Then I proceeded to one hot encode the categorical variables to allow for logistic regression to use the features as numerical values. There were no missing values in the dataset so I did not need to clean the dataset. I split the data into training and testing data with a split of 80% training and 20% test. Then, I normalized the data such that the training data had 0 mean and std of 1. I saved these values and applied them to the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000</td>\n",
       "      <td>F</td>\n",
       "      <td>University</td>\n",
       "      <td>Married</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120000</td>\n",
       "      <td>F</td>\n",
       "      <td>University</td>\n",
       "      <td>Single</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90000</td>\n",
       "      <td>F</td>\n",
       "      <td>University</td>\n",
       "      <td>Single</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50000</td>\n",
       "      <td>F</td>\n",
       "      <td>University</td>\n",
       "      <td>Married</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50000</td>\n",
       "      <td>M</td>\n",
       "      <td>University</td>\n",
       "      <td>Married</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LIMIT_BAL SEX   EDUCATION MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  PAY_5  \\\n",
       "0      20000   F  University  Married   24      2      2     -1     -1     -2   \n",
       "1     120000   F  University   Single   26     -1      2      0      0      0   \n",
       "2      90000   F  University   Single   34      0      0      0      0      0   \n",
       "3      50000   F  University  Married   37      0      0      0      0      0   \n",
       "4      50000   M  University  Married   57     -1      0     -1      0      0   \n",
       "\n",
       "   ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0  ...          0          0          0         0       689         0   \n",
       "1  ...       3272       3455       3261         0      1000      1000   \n",
       "2  ...      14331      14948      15549      1518      1500      1000   \n",
       "3  ...      28314      28959      29547      2000      2019      1200   \n",
       "4  ...      20940      19146      19131      2000     36681     10000   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default   \n",
       "0         0         0         0         Y  \n",
       "1      1000         0      2000         Y  \n",
       "2      1000      1000      5000         N  \n",
       "3      1100      1069      1000         N  \n",
       "4      9000       689       679         N  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "folder_name = \"./credit-card-defaulter-prediction\"\n",
    "file_name = \"Credit Card Defaulter Prediction.csv\"\n",
    "\n",
    "\n",
    "if not os.path.exists(folder_name):\n",
    "    import opendatasets as od\n",
    "    # download the data\n",
    "    dataset_link = \"https://www.kaggle.com/datasets/gauravtopre/credit-card-defaulter-prediction\"\n",
    "    od.download(dataset_link)\n",
    "\n",
    "data = pd.read_csv(os.path.join(folder_name, file_name))\n",
    "data = data.drop('ID', axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_labels =  ['EDUCATION', 'MARRIAGE', 'SEX'] + ['PAY_{}'.format(x) for x in [0,2,3,4,6]] + ['default ']\n",
    "y_label = 'default '\n",
    "\n",
    "train_proportion = 0.8\n",
    "cutoff = int(len(data) * train_proportion)\n",
    "\n",
    "\n",
    "# one hot encode the categorical variables\n",
    "def get_mapping(column):\n",
    "    unique_matches = set()\n",
    "\n",
    "    for item in column:\n",
    "        unique_matches.add(item)\n",
    "\n",
    "    mapping = {}\n",
    "    for i, item in enumerate(unique_matches):\n",
    "        mapping[item] = i\n",
    "\n",
    "    return mapping\n",
    "    \n",
    "def transform(mapping, column):\n",
    "    out = np.zeros((len(mapping), len(column)), dtype=np.float32)\n",
    "    for i, item in enumerate(column):\n",
    "        out[mapping[item]][i] = 1.0\n",
    "\n",
    "    return out\n",
    "\n",
    "for label in one_hot_labels:\n",
    "    column = data.loc[:,label]\n",
    "    data = data.drop(label, axis=1)\n",
    "    mapping = get_mapping(column)\n",
    "    new_labels = ['{}_{}'.format(label, index) for index in range(len(mapping))]\n",
    "    for label, encoding in zip(new_labels, transform(mapping, column)):\n",
    "        data[label] = encoding\n",
    "\n",
    "# randomly separate testing / training data\n",
    "numeric_data = data.to_numpy()\n",
    "np.random.shuffle(numeric_data)\n",
    "train_data_x = numeric_data[0:cutoff,:-1]\n",
    "train_data_y = numeric_data[0:cutoff,-1]\n",
    "test_data_x = numeric_data[cutoff:,:-1]\n",
    "test_data_y = numeric_data[cutoff:,-1]\n",
    "\n",
    "# -- normalize the data --\n",
    "mean = np.mean(train_data_x, axis=0)\n",
    "std = np.std(train_data_x, axis=0)\n",
    "\n",
    "train_data = np.divide(train_data_x - mean, std), train_data_y\n",
    "test_data = np.divide(test_data_x - mean, std), test_data_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "**Implement, Logistic Regression in this step. Clearly write your cost function and derivatives before implementing gradient descent. Do not use any built-in packages for this step. You can use the vectorization techniques demonstrated in class. Implement any 2 variants of gradient descent in their original form. (Refer to the research paper discussed in class).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class LogisticRegressionModel:\n",
    "    def __init__(self, cost_fn, input_features):\n",
    "        self.cost_fn = cost_fn\n",
    "        self.weights = np.zeros((input_features, 1))\n",
    "        self.bias = np.zeros((1,))\n",
    "\n",
    "    def _calculate_gradient(self, X, Y_hat, Y):\n",
    "        return (\n",
    "            np.dot(X.T, (Y_hat-Y).T) / Y.shape[0], \n",
    "            np.sum(Y_hat-Y) / Y.shape[0]\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return sigmoid(np.dot(self.weights.T, X.T) + self.bias)\n",
    "\n",
    "    def gradient_update(self, learning_rate, X, Y_hat, Y):\n",
    "        dw, db = self._calculate_gradient(X, Y_hat, Y)\n",
    "\n",
    "        self.weights -= dw * learning_rate\n",
    "        self.bias -= db * learning_rate\n",
    "\n",
    "    def _calculate_loss(self, Y_hat, Y):\n",
    "        return self.cost_fn(Y_hat, Y)\n",
    "\n",
    "    def evaluate(self, dataset):\n",
    "        X, Y = dataset\n",
    "        Y_hat = self.forward(X)\n",
    "        return self._calculate_loss(Y_hat, Y)\n",
    "\n",
    "def cost_fn(Y_hat, Y):\n",
    "    assert Y_hat.flatten().shape == Y.flatten().shape\n",
    "    return (\n",
    "        -1 / Y_hat.shape[0] \n",
    "        * np.sum(Y * np.log(Y_hat) \n",
    "        + (1 - Y) * (np.log(1 - Y_hat)))\n",
    "    )\n",
    "    \n",
    "def fit_sgd(dataset_train, dataset_test, epochs, learning_rate, input_features):\n",
    "    model = LogisticRegressionModel(cost_fn=cost_fn, input_features=input_features)\n",
    "    X_train, Y_train = dataset_train\n",
    "\n",
    "    history = []\n",
    "\n",
    "    for _ in tqdm(range(epochs)):\n",
    "        for i in range(len(X_train)):\n",
    "            X = np.array([X_train[i]])\n",
    "            Y = np.array([Y_train[i]])\n",
    "            Y_hat = model.forward(X)\n",
    "            model.gradient_update(learning_rate, X, Y_hat, Y)\n",
    "\n",
    "        history.append(model.evaluate(dataset_test))\n",
    "\n",
    "    return history, model.evaluate(dataset_test)\n",
    "\n",
    "\n",
    "\n",
    "def fit_batched(dataset_train, dataset_test, epochs, learning_rate, input_features):\n",
    "    model = LogisticRegressionModel(cost_fn=cost_fn, input_features=input_features)\n",
    "    X, Y = dataset_train\n",
    "\n",
    "    history = []\n",
    "\n",
    "    for _ in tqdm(range(epochs)):\n",
    "        Y_hat = model.forward(X)\n",
    "        model.gradient_update(learning_rate, X, Y_hat, Y)\n",
    "\n",
    "        history.append(model.evaluate(dataset_test))\n",
    "\n",
    "    return history, model.evaluate(dataset_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:08<00:00, 617.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.578508670661842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f89e29dfd90>]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 5000\n",
    "history, loss = fit_batched(train_data, test_data, epochs, 1E-1, train_data[0].shape[1])\n",
    "\n",
    "print(loss)\n",
    "plt.plot(range(epochs), history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:27<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.916530374229986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f89e2f07910>]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 30\n",
    "history, loss = fit_sgd(train_data, test_data, epochs, 1E-3, train_data[0].shape[1])\n",
    "\n",
    "print(loss)\n",
    "plt.plot(range(epochs), history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "**Pick any 2 optimization algorithms that are used to optimize the ‘vanilla’ gradient descent. Implement both. You may implement these algorithms yourself OR use a package. In your conclusion, compare both optimization techniques/algorithms with respect to the results you achieve. Also \n",
    "compare these results with the original implementation of gradient descent (Task 3 above). Describe why or why not should we use optimization algorithms for the task at hand.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for the optimizers seems to slightly outperform the the standard gradient decent algorithm. However, in training the models the ones with optimizers seem to be more resilient to small hyperparameter changes. For example, in the SGD optimization for task 3 a learning rate of >=1E-2 would result in a exploding gradient. However, to achieve the results the batched gradient decent seemed to be the fastest method to converge to comperable results with the rest of the models (8 seconds). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "def get_model(dataset, optimizer):\n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(1, input_shape=(dataset.shape[1],), activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "model_adam = get_model(train_data[0], Adam(learning_rate=1E4))\n",
    "model_rms = get_model(train_data[0], RMSprop(learning_rate=1E4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/30\n",
      "24000/24000 [==============================] - 2s 100us/sample - loss: 14535.8306 - val_loss: 7914.9377\n",
      "Epoch 2/30\n",
      "24000/24000 [==============================] - 1s 52us/sample - loss: 5030.1555 - val_loss: 2533.9880\n",
      "Epoch 3/30\n",
      "24000/24000 [==============================] - 3s 108us/sample - loss: 2214.8532 - val_loss: 1835.5588\n",
      "Epoch 4/30\n",
      "24000/24000 [==============================] - 2s 71us/sample - loss: 1725.3165 - val_loss: 1465.0818\n",
      "Epoch 5/30\n",
      "24000/24000 [==============================] - 1s 50us/sample - loss: 1400.3673 - val_loss: 1197.8580\n",
      "Epoch 6/30\n",
      "24000/24000 [==============================] - 1s 47us/sample - loss: 1136.5743 - val_loss: 974.8214\n",
      "Epoch 7/30\n",
      "24000/24000 [==============================] - 1s 51us/sample - loss: 909.1082 - val_loss: 783.5745\n",
      "Epoch 8/30\n",
      "24000/24000 [==============================] - 1s 47us/sample - loss: 723.2540 - val_loss: 633.4319\n",
      "Epoch 9/30\n",
      "24000/24000 [==============================] - 1s 48us/sample - loss: 586.1209 - val_loss: 516.1235\n",
      "Epoch 10/30\n",
      "24000/24000 [==============================] - 1s 53us/sample - loss: 471.7656 - val_loss: 405.9686\n",
      "Epoch 11/30\n",
      "24000/24000 [==============================] - 1s 54us/sample - loss: 360.6562 - val_loss: 300.2993\n",
      "Epoch 12/30\n",
      "24000/24000 [==============================] - 1s 55us/sample - loss: 253.8443 - val_loss: 199.8890\n",
      "Epoch 13/30\n",
      "24000/24000 [==============================] - 1s 53us/sample - loss: 152.7594 - val_loss: 108.1034\n",
      "Epoch 14/30\n",
      "24000/24000 [==============================] - 1s 55us/sample - loss: 61.8172 - val_loss: 20.1992\n",
      "Epoch 15/30\n",
      "24000/24000 [==============================] - 1s 52us/sample - loss: 10.3363 - val_loss: 3.6771\n",
      "Epoch 16/30\n",
      "24000/24000 [==============================] - 1s 49us/sample - loss: 5.6589 - val_loss: 2.2821\n",
      "Epoch 17/30\n",
      "24000/24000 [==============================] - 1s 51us/sample - loss: 5.2295 - val_loss: 1.8359\n",
      "Epoch 18/30\n",
      "24000/24000 [==============================] - 2s 63us/sample - loss: 5.4734 - val_loss: 2.9842\n",
      "Epoch 19/30\n",
      "24000/24000 [==============================] - 2s 65us/sample - loss: 4.4451 - val_loss: 3.2303\n",
      "Epoch 20/30\n",
      "24000/24000 [==============================] - 1s 50us/sample - loss: 4.5470 - val_loss: 4.2879\n",
      "Epoch 21/30\n",
      "24000/24000 [==============================] - 1s 47us/sample - loss: 4.6833 - val_loss: 9.1452\n",
      "Epoch 22/30\n",
      "24000/24000 [==============================] - 1s 54us/sample - loss: 4.5192 - val_loss: 13.6041\n",
      "Epoch 23/30\n",
      "24000/24000 [==============================] - 1s 50us/sample - loss: 4.3202 - val_loss: 1.9211\n",
      "Epoch 24/30\n",
      "24000/24000 [==============================] - 1s 61us/sample - loss: 3.9290 - val_loss: 8.3484\n",
      "Epoch 25/30\n",
      "24000/24000 [==============================] - 1s 49us/sample - loss: 4.3856 - val_loss: 3.1904\n",
      "Epoch 26/30\n",
      "24000/24000 [==============================] - 1s 51us/sample - loss: 4.0496 - val_loss: 1.4137\n",
      "Epoch 27/30\n",
      "24000/24000 [==============================] - 1s 48us/sample - loss: 4.4622 - val_loss: 7.3952\n",
      "Epoch 28/30\n",
      "24000/24000 [==============================] - 1s 51us/sample - loss: 4.6000 - val_loss: 4.6493\n",
      "Epoch 29/30\n",
      "24000/24000 [==============================] - 1s 50us/sample - loss: 3.8335 - val_loss: 20.4737\n",
      "Epoch 30/30\n",
      "24000/24000 [==============================] - 1s 47us/sample - loss: 5.3282 - val_loss: 6.1847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8a8c729450>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_adam.fit(x=train_data[0], y=train_data[1], epochs=30, batch_size=32, validation_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/30\n",
      "24000/24000 [==============================] - 6s 254us/sample - loss: 15277.8948 - val_loss: 9093.1571\n",
      "Epoch 2/30\n",
      "24000/24000 [==============================] - 3s 127us/sample - loss: 6113.3312 - val_loss: 3568.5807\n",
      "Epoch 3/30\n",
      "24000/24000 [==============================] - 2s 104us/sample - loss: 3124.5484 - val_loss: 2569.9029\n",
      "Epoch 4/30\n",
      "24000/24000 [==============================] - 2s 63us/sample - loss: 2269.9909 - val_loss: 1858.7221\n",
      "Epoch 5/30\n",
      "24000/24000 [==============================] - 1s 61us/sample - loss: 1699.9077 - val_loss: 1418.1838\n",
      "Epoch 6/30\n",
      "24000/24000 [==============================] - 1s 59us/sample - loss: 1361.3401 - val_loss: 1161.3027\n",
      "Epoch 7/30\n",
      "24000/24000 [==============================] - 1s 60us/sample - loss: 1165.8984 - val_loss: 1001.3409\n",
      "Epoch 8/30\n",
      "24000/24000 [==============================] - 1s 54us/sample - loss: 1024.6627 - val_loss: 873.5212\n",
      "Epoch 9/30\n",
      "24000/24000 [==============================] - 2s 89us/sample - loss: 907.6539 - val_loss: 777.6161\n",
      "Epoch 10/30\n",
      "24000/24000 [==============================] - 2s 70us/sample - loss: 812.2043 - val_loss: 693.7847\n",
      "Epoch 11/30\n",
      "24000/24000 [==============================] - 2s 97us/sample - loss: 728.2486 - val_loss: 617.9543\n",
      "Epoch 12/30\n",
      "24000/24000 [==============================] - 2s 63us/sample - loss: 652.8267 - val_loss: 553.0243\n",
      "Epoch 13/30\n",
      "24000/24000 [==============================] - 2s 65us/sample - loss: 582.9998 - val_loss: 490.7914\n",
      "Epoch 14/30\n",
      "24000/24000 [==============================] - 1s 61us/sample - loss: 521.3020 - val_loss: 438.4766\n",
      "Epoch 15/30\n",
      "24000/24000 [==============================] - 2s 83us/sample - loss: 463.4822 - val_loss: 395.0399\n",
      "Epoch 16/30\n",
      "24000/24000 [==============================] - 2s 96us/sample - loss: 410.7472 - val_loss: 341.6255\n",
      "Epoch 17/30\n",
      "24000/24000 [==============================] - 2s 91us/sample - loss: 359.3153 - val_loss: 296.1586\n",
      "Epoch 18/30\n",
      "24000/24000 [==============================] - 2s 68us/sample - loss: 311.0000 - val_loss: 255.3748\n",
      "Epoch 19/30\n",
      "24000/24000 [==============================] - 1s 61us/sample - loss: 263.6148 - val_loss: 214.5317\n",
      "Epoch 20/30\n",
      "24000/24000 [==============================] - 2s 67us/sample - loss: 217.4348 - val_loss: 175.0153\n",
      "Epoch 21/30\n",
      "24000/24000 [==============================] - 2s 92us/sample - loss: 173.8709 - val_loss: 138.6277\n",
      "Epoch 22/30\n",
      "24000/24000 [==============================] - 2s 69us/sample - loss: 132.5987 - val_loss: 95.0890\n",
      "Epoch 23/30\n",
      "24000/24000 [==============================] - 2s 82us/sample - loss: 93.4571 - val_loss: 80.3068\n",
      "Epoch 24/30\n",
      "24000/24000 [==============================] - 4s 168us/sample - loss: 57.9181 - val_loss: 32.4277\n",
      "Epoch 25/30\n",
      "24000/24000 [==============================] - 2s 64us/sample - loss: 28.8744 - val_loss: 6.5183\n",
      "Epoch 26/30\n",
      "24000/24000 [==============================] - 1s 59us/sample - loss: 17.0431 - val_loss: 14.3937\n",
      "Epoch 27/30\n",
      "24000/24000 [==============================] - 1s 59us/sample - loss: 16.6458 - val_loss: 11.6220\n",
      "Epoch 28/30\n",
      "24000/24000 [==============================] - 2s 85us/sample - loss: 16.6796 - val_loss: 26.6044\n",
      "Epoch 29/30\n",
      "24000/24000 [==============================] - 3s 130us/sample - loss: 16.9000 - val_loss: 6.5237\n",
      "Epoch 30/30\n",
      "24000/24000 [==============================] - 1s 57us/sample - loss: 16.7307 - val_loss: 44.9626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8a371641d0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rms.fit(x=train_data[0], y=train_data[1], epochs=30, batch_size=32, validation_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('python-chess-lib')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "946d23f443fb1dfa39329254e79dc1d17639e1dd0ec90f8c9f5045c0d1ab1f3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

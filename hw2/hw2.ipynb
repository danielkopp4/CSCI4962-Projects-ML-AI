{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 \n",
    "**Describe a machine learning problem that you would like to solve using Logistic Regression. Clearly state why Logistic regression is the best choice for solving this problem.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of predicting credit defaults is an are where logistic regression can thrive. We must be able to predict the likelihood of an individual to default on their credit card. There are many variables to calculate the credit default probability. Thus, logistic regression would be the ideal choice to predict this probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "**Pick a publicly available dataset (except The Titanic Dataset because we already discussed that in class) that you will use to solve this problem. You must provide a link to the dataset and perform necessary Exploratory Data Analysis (EDA). Clearly demonstrate the steps you follow for your EDA with a justification of why these were required. For example, if the dataset has lot of missing values, then why did you use a specific technique when handling missing data. This task may include data visualization (Check this link : https://www.geeksforgeeks.org/top-8-python-libraries-for-data-visualization/).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that is used is from this link: https://www.kaggle.com/datasets/gauravtopre/credit-card-defaulter-prediction.\n",
    "\n",
    "Before I preprocessed the data, I removed the `ID` column as it is not relevant to the data. Then I proceeded to one hot encode the categorical variables to allow for logistic regression to use the features as numerical values. There were no missing values in the dataset so I did not need to clean the dataset. I split the data into training and testing data with a split of 80% training and 20% test. Then, I normalized the data such that the training data had 0 mean and std of 1. I saved these values and applied them to the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000</td>\n",
       "      <td>F</td>\n",
       "      <td>University</td>\n",
       "      <td>Married</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120000</td>\n",
       "      <td>F</td>\n",
       "      <td>University</td>\n",
       "      <td>Single</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90000</td>\n",
       "      <td>F</td>\n",
       "      <td>University</td>\n",
       "      <td>Single</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50000</td>\n",
       "      <td>F</td>\n",
       "      <td>University</td>\n",
       "      <td>Married</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50000</td>\n",
       "      <td>M</td>\n",
       "      <td>University</td>\n",
       "      <td>Married</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LIMIT_BAL SEX   EDUCATION MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  PAY_5  \\\n",
       "0      20000   F  University  Married   24      2      2     -1     -1     -2   \n",
       "1     120000   F  University   Single   26     -1      2      0      0      0   \n",
       "2      90000   F  University   Single   34      0      0      0      0      0   \n",
       "3      50000   F  University  Married   37      0      0      0      0      0   \n",
       "4      50000   M  University  Married   57     -1      0     -1      0      0   \n",
       "\n",
       "   ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0  ...          0          0          0         0       689         0   \n",
       "1  ...       3272       3455       3261         0      1000      1000   \n",
       "2  ...      14331      14948      15549      1518      1500      1000   \n",
       "3  ...      28314      28959      29547      2000      2019      1200   \n",
       "4  ...      20940      19146      19131      2000     36681     10000   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default   \n",
       "0         0         0         0         Y  \n",
       "1      1000         0      2000         Y  \n",
       "2      1000      1000      5000         N  \n",
       "3      1100      1069      1000         N  \n",
       "4      9000       689       679         N  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "folder_name = \"./credit-card-defaulter-prediction\"\n",
    "file_name = \"Credit Card Defaulter Prediction.csv\"\n",
    "\n",
    "\n",
    "if not os.path.exists(folder_name):\n",
    "    import opendatasets as od\n",
    "    # download the data\n",
    "    dataset_link = \"https://www.kaggle.com/datasets/gauravtopre/credit-card-defaulter-prediction\"\n",
    "    od.download(dataset_link)\n",
    "\n",
    "data = pd.read_csv(os.path.join(folder_name, file_name))\n",
    "data = data.drop('ID', axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_labels =  ['EDUCATION', 'MARRIAGE', 'SEX'] + ['PAY_{}'.format(x) for x in [0,2,3,4,6]] + ['default ']\n",
    "y_label = 'default '\n",
    "\n",
    "train_proportion = 0.8\n",
    "cutoff = int(len(data) * train_proportion)\n",
    "\n",
    "\n",
    "# one hot encode the categorical variables\n",
    "def get_mapping(column):\n",
    "    unique_matches = set()\n",
    "\n",
    "    for item in column:\n",
    "        unique_matches.add(item)\n",
    "\n",
    "    mapping = {}\n",
    "    for i, item in enumerate(unique_matches):\n",
    "        mapping[item] = i\n",
    "\n",
    "    return mapping\n",
    "    \n",
    "def transform(mapping, column):\n",
    "    out = np.zeros((len(mapping), len(column)), dtype=np.float32)\n",
    "    for i, item in enumerate(column):\n",
    "        out[mapping[item]][i] = 1.0\n",
    "\n",
    "    return out\n",
    "\n",
    "for label in one_hot_labels:\n",
    "    column = data.loc[:,label]\n",
    "    data = data.drop(label, axis=1)\n",
    "    mapping = get_mapping(column)\n",
    "    new_labels = ['{}_{}'.format(label, index) for index in range(len(mapping))]\n",
    "    for label, encoding in zip(new_labels, transform(mapping, column)):\n",
    "        data[label] = encoding\n",
    "\n",
    "# randomly separate testing / training data\n",
    "numeric_data = data.to_numpy()\n",
    "np.random.shuffle(numeric_data)\n",
    "train_data_x = numeric_data[0:cutoff,:-1]\n",
    "train_data_y = numeric_data[0:cutoff,-1]\n",
    "test_data_x = numeric_data[cutoff:,:-1]\n",
    "test_data_y = numeric_data[cutoff:,-1]\n",
    "\n",
    "# -- normalize the data --\n",
    "mean = np.mean(train_data_x, axis=0)\n",
    "std = np.std(train_data_x, axis=0)\n",
    "\n",
    "train_data = np.divide(train_data_x - mean, std), train_data_y\n",
    "test_data = np.divide(test_data_x - mean, std), test_data_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "**Implement, Logistic Regression in this step. Clearly write your cost function and derivatives before implementing gradient descent. Do not use any built-in packages for this step. You can use the vectorization techniques demonstrated in class. Implement any 2 variants of gradient descent in their original form. (Refer to the research paper discussed in class).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class LogisticRegressionModel:\n",
    "    def __init__(self, cost_fn, input_features):\n",
    "        self.cost_fn = cost_fn\n",
    "        self.weights = np.zeros((input_features, 1))\n",
    "        self.bias = np.zeros((1,))\n",
    "\n",
    "    def _calculate_gradient(self, X, Y_hat, Y):\n",
    "        return (\n",
    "            np.dot(X.T, (Y_hat-Y).T) / Y.shape[0], \n",
    "            np.sum(Y_hat-Y) / Y.shape[0]\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return sigmoid(np.dot(self.weights.T, X.T) + self.bias)\n",
    "\n",
    "    def gradient_update(self, learning_rate, X, Y_hat, Y):\n",
    "        dw, db = self._calculate_gradient(X, Y_hat, Y)\n",
    "\n",
    "        self.weights -= dw * learning_rate\n",
    "        self.bias -= db * learning_rate\n",
    "\n",
    "    def _calculate_loss(self, Y_hat, Y):\n",
    "        return self.cost_fn(Y_hat, Y)\n",
    "\n",
    "    def evaluate(self, dataset):\n",
    "        X, Y = dataset\n",
    "        Y_hat = self.forward(X)\n",
    "        return self._calculate_loss(Y_hat, Y)\n",
    "\n",
    "def cost_fn(Y_hat, Y):\n",
    "    assert Y_hat.flatten().shape == Y.flatten().shape\n",
    "    return (\n",
    "        -1 / Y_hat.shape[0] \n",
    "        * np.sum(Y * np.log(Y_hat) \n",
    "        + (1 - Y) * (np.log(1 - Y_hat)))\n",
    "    )\n",
    "    \n",
    "def fit_sgd(dataset_train, dataset_test, epochs, learning_rate, input_features):\n",
    "    model = LogisticRegressionModel(cost_fn=cost_fn, input_features=input_features)\n",
    "    X_train, Y_train = dataset_train\n",
    "\n",
    "    history = []\n",
    "\n",
    "    for _ in tqdm(range(epochs)):\n",
    "        for i in range(len(X_train)):\n",
    "            X = np.array([X_train[i]])\n",
    "            Y = np.array([Y_train[i]])\n",
    "            Y_hat = model.forward(X)\n",
    "            model.gradient_update(learning_rate, X, Y_hat, Y)\n",
    "\n",
    "        history.append(model.evaluate(dataset_test))\n",
    "\n",
    "    return history, model.evaluate(dataset_test)\n",
    "\n",
    "\n",
    "\n",
    "def fit_batched(dataset_train, dataset_test, epochs, learning_rate, input_features):\n",
    "    model = LogisticRegressionModel(cost_fn=cost_fn, input_features=input_features)\n",
    "    X, Y = dataset_train\n",
    "\n",
    "    history = []\n",
    "\n",
    "    for _ in tqdm(range(epochs)):\n",
    "        Y_hat = model.forward(X)\n",
    "        model.gradient_update(learning_rate, X, Y_hat, Y)\n",
    "\n",
    "        history.append(model.evaluate(dataset_test))\n",
    "\n",
    "    return history, model.evaluate(dataset_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:07<00:00, 679.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.121797580377013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9dbf7decd0>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 5000\n",
    "history, loss = fit_batched(train_data, test_data, epochs, 1E-1, train_data[0].shape[1])\n",
    "\n",
    "print(loss)\n",
    "plt.plot(range(epochs), history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:26<00:00,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.887534700514072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9dbdb99e50>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 30\n",
    "history, loss = fit_sgd(train_data, test_data, epochs, 1E-3, train_data[0].shape[1])\n",
    "\n",
    "print(loss)\n",
    "plt.plot(range(epochs), history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "**Pick any 2 optimization algorithms that are used to optimize the â€˜vanillaâ€™ gradient descent. Implement both. You may implement these algorithms yourself OR use a package. In your conclusion, compare both optimization techniques/algorithms with respect to the results you achieve. Also \n",
    "compare these results with the original implementation of gradient descent (Task 3 above). Describe why or why not should we use optimization algorithms for the task at hand.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for the optimizers seems to slightly outperform the the standard gradient decent algorithm. However, in training the models the ones with optimizers seem to be more resilient to small hyperparameter changes. For example, in the SGD optimization for task 3 a learning rate of >=1E-2 would result in a exploding gradient. However, to achieve the results the batched gradient decent seemed to be the fastest method to converge to comperable results with the rest of the models (8 seconds). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "def get_model(dataset, optimizer):\n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(1, input_shape=(dataset.shape[1],), activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "model_adam = get_model(train_data[0], Adam(learning_rate=1E4))\n",
    "model_rms = get_model(train_data[0], RMSprop(learning_rate=1E4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/30\n",
      "24000/24000 [==============================] - 2s 100us/sample - loss: 14535.8306 - val_loss: 7914.9377\n",
      "Epoch 2/30\n",
      "24000/24000 [==============================] - 1s 52us/sample - loss: 5030.1555 - val_loss: 2533.9880\n",
      "Epoch 3/30\n",
      "24000/24000 [==============================] - 3s 108us/sample - loss: 2214.8532 - val_loss: 1835.5588\n",
      "Epoch 4/30\n",
      "24000/24000 [==============================] - 2s 71us/sample - loss: 1725.3165 - val_loss: 1465.0818\n",
      "Epoch 5/30\n",
      "24000/24000 [==============================] - 1s 50us/sample - loss: 1400.3673 - val_loss: 1197.8580\n",
      "Epoch 6/30\n",
      "24000/24000 [==============================] - 1s 47us/sample - loss: 1136.5743 - val_loss: 974.8214\n",
      "Epoch 7/30\n",
      "24000/24000 [==============================] - 1s 51us/sample - loss: 909.1082 - val_loss: 783.5745\n",
      "Epoch 8/30\n",
      "24000/24000 [==============================] - 1s 47us/sample - loss: 723.2540 - val_loss: 633.4319\n",
      "Epoch 9/30\n",
      "24000/24000 [==============================] - 1s 48us/sample - loss: 586.1209 - val_loss: 516.1235\n",
      "Epoch 10/30\n",
      "24000/24000 [==============================] - 1s 53us/sample - loss: 471.7656 - val_loss: 405.9686\n",
      "Epoch 11/30\n",
      "24000/24000 [==============================] - 1s 54us/sample - loss: 360.6562 - val_loss: 300.2993\n",
      "Epoch 12/30\n",
      "24000/24000 [==============================] - 1s 55us/sample - loss: 253.8443 - val_loss: 199.8890\n",
      "Epoch 13/30\n",
      "24000/24000 [==============================] - 1s 53us/sample - loss: 152.7594 - val_loss: 108.1034\n",
      "Epoch 14/30\n",
      "24000/24000 [==============================] - 1s 55us/sample - loss: 61.8172 - val_loss: 20.1992\n",
      "Epoch 15/30\n",
      "24000/24000 [==============================] - 1s 52us/sample - loss: 10.3363 - val_loss: 3.6771\n",
      "Epoch 16/30\n",
      "24000/24000 [==============================] - 1s 49us/sample - loss: 5.6589 - val_loss: 2.2821\n",
      "Epoch 17/30\n",
      "24000/24000 [==============================] - 1s 51us/sample - loss: 5.2295 - val_loss: 1.8359\n",
      "Epoch 18/30\n",
      "24000/24000 [==============================] - 2s 63us/sample - loss: 5.4734 - val_loss: 2.9842\n",
      "Epoch 19/30\n",
      "24000/24000 [==============================] - 2s 65us/sample - loss: 4.4451 - val_loss: 3.2303\n",
      "Epoch 20/30\n",
      "24000/24000 [==============================] - 1s 50us/sample - loss: 4.5470 - val_loss: 4.2879\n",
      "Epoch 21/30\n",
      "24000/24000 [==============================] - 1s 47us/sample - loss: 4.6833 - val_loss: 9.1452\n",
      "Epoch 22/30\n",
      "24000/24000 [==============================] - 1s 54us/sample - loss: 4.5192 - val_loss: 13.6041\n",
      "Epoch 23/30\n",
      "24000/24000 [==============================] - 1s 50us/sample - loss: 4.3202 - val_loss: 1.9211\n",
      "Epoch 24/30\n",
      "24000/24000 [==============================] - 1s 61us/sample - loss: 3.9290 - val_loss: 8.3484\n",
      "Epoch 25/30\n",
      "24000/24000 [==============================] - 1s 49us/sample - loss: 4.3856 - val_loss: 3.1904\n",
      "Epoch 26/30\n",
      "24000/24000 [==============================] - 1s 51us/sample - loss: 4.0496 - val_loss: 1.4137\n",
      "Epoch 27/30\n",
      "24000/24000 [==============================] - 1s 48us/sample - loss: 4.4622 - val_loss: 7.3952\n",
      "Epoch 28/30\n",
      "24000/24000 [==============================] - 1s 51us/sample - loss: 4.6000 - val_loss: 4.6493\n",
      "Epoch 29/30\n",
      "24000/24000 [==============================] - 1s 50us/sample - loss: 3.8335 - val_loss: 20.4737\n",
      "Epoch 30/30\n",
      "24000/24000 [==============================] - 1s 47us/sample - loss: 5.3282 - val_loss: 6.1847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8a8c729450>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_adam.fit(x=train_data[0], y=train_data[1], epochs=30, batch_size=32, validation_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/30\n",
      "24000/24000 [==============================] - 6s 254us/sample - loss: 15277.8948 - val_loss: 9093.1571\n",
      "Epoch 2/30\n",
      "24000/24000 [==============================] - 3s 127us/sample - loss: 6113.3312 - val_loss: 3568.5807\n",
      "Epoch 3/30\n",
      "24000/24000 [==============================] - 2s 104us/sample - loss: 3124.5484 - val_loss: 2569.9029\n",
      "Epoch 4/30\n",
      "24000/24000 [==============================] - 2s 63us/sample - loss: 2269.9909 - val_loss: 1858.7221\n",
      "Epoch 5/30\n",
      "24000/24000 [==============================] - 1s 61us/sample - loss: 1699.9077 - val_loss: 1418.1838\n",
      "Epoch 6/30\n",
      "24000/24000 [==============================] - 1s 59us/sample - loss: 1361.3401 - val_loss: 1161.3027\n",
      "Epoch 7/30\n",
      "24000/24000 [==============================] - 1s 60us/sample - loss: 1165.8984 - val_loss: 1001.3409\n",
      "Epoch 8/30\n",
      "24000/24000 [==============================] - 1s 54us/sample - loss: 1024.6627 - val_loss: 873.5212\n",
      "Epoch 9/30\n",
      "24000/24000 [==============================] - 2s 89us/sample - loss: 907.6539 - val_loss: 777.6161\n",
      "Epoch 10/30\n",
      "24000/24000 [==============================] - 2s 70us/sample - loss: 812.2043 - val_loss: 693.7847\n",
      "Epoch 11/30\n",
      "24000/24000 [==============================] - 2s 97us/sample - loss: 728.2486 - val_loss: 617.9543\n",
      "Epoch 12/30\n",
      "24000/24000 [==============================] - 2s 63us/sample - loss: 652.8267 - val_loss: 553.0243\n",
      "Epoch 13/30\n",
      "24000/24000 [==============================] - 2s 65us/sample - loss: 582.9998 - val_loss: 490.7914\n",
      "Epoch 14/30\n",
      "24000/24000 [==============================] - 1s 61us/sample - loss: 521.3020 - val_loss: 438.4766\n",
      "Epoch 15/30\n",
      "24000/24000 [==============================] - 2s 83us/sample - loss: 463.4822 - val_loss: 395.0399\n",
      "Epoch 16/30\n",
      "24000/24000 [==============================] - 2s 96us/sample - loss: 410.7472 - val_loss: 341.6255\n",
      "Epoch 17/30\n",
      "24000/24000 [==============================] - 2s 91us/sample - loss: 359.3153 - val_loss: 296.1586\n",
      "Epoch 18/30\n",
      "24000/24000 [==============================] - 2s 68us/sample - loss: 311.0000 - val_loss: 255.3748\n",
      "Epoch 19/30\n",
      "24000/24000 [==============================] - 1s 61us/sample - loss: 263.6148 - val_loss: 214.5317\n",
      "Epoch 20/30\n",
      "24000/24000 [==============================] - 2s 67us/sample - loss: 217.4348 - val_loss: 175.0153\n",
      "Epoch 21/30\n",
      "24000/24000 [==============================] - 2s 92us/sample - loss: 173.8709 - val_loss: 138.6277\n",
      "Epoch 22/30\n",
      "24000/24000 [==============================] - 2s 69us/sample - loss: 132.5987 - val_loss: 95.0890\n",
      "Epoch 23/30\n",
      "24000/24000 [==============================] - 2s 82us/sample - loss: 93.4571 - val_loss: 80.3068\n",
      "Epoch 24/30\n",
      "24000/24000 [==============================] - 4s 168us/sample - loss: 57.9181 - val_loss: 32.4277\n",
      "Epoch 25/30\n",
      "24000/24000 [==============================] - 2s 64us/sample - loss: 28.8744 - val_loss: 6.5183\n",
      "Epoch 26/30\n",
      "24000/24000 [==============================] - 1s 59us/sample - loss: 17.0431 - val_loss: 14.3937\n",
      "Epoch 27/30\n",
      "24000/24000 [==============================] - 1s 59us/sample - loss: 16.6458 - val_loss: 11.6220\n",
      "Epoch 28/30\n",
      "24000/24000 [==============================] - 2s 85us/sample - loss: 16.6796 - val_loss: 26.6044\n",
      "Epoch 29/30\n",
      "24000/24000 [==============================] - 3s 130us/sample - loss: 16.9000 - val_loss: 6.5237\n",
      "Epoch 30/30\n",
      "24000/24000 [==============================] - 1s 57us/sample - loss: 16.7307 - val_loss: 44.9626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8a371641d0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rms.fit(x=train_data[0], y=train_data[1], epochs=30, batch_size=32, validation_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1.0\n",
      "test 1.0\n"
     ]
    }
   ],
   "source": [
    "dt = tree.DecisionTreeClassifier()\n",
    "\n",
    "dt.fit(train_data[0], train_data[1])\n",
    "print(\"train\", accuracy_score(train_data[1], dt.predict(train_data[0])))\n",
    "print(\"test\", accuracy_score(test_data[1], dt.predict(test_data[0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunatly, the decision tree classifier is already at 100% accuracy so we cannot see how changing the parameters aid in overfitting or how we can improve the variance. We can change the parameters to make the model worse. The only parameter that I could change that affects either of the accuracies was the max_features. The reason this affects the accuracy is quite intuitive as we are simply reducing `max_features` to 1 which reduces the available information to the model. It is therefore not able to find a pattern to the data. It Looks from the results below that the variance increased as the test accuracy by doing this but more likely the bias increased and we are unable to see the effect of that as the accuracy is too high to see an effect. We cannot know for sure though unless we chose a different dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1.0\n",
      "test 0.8411666666666666\n"
     ]
    }
   ],
   "source": [
    "dt = tree.DecisionTreeClassifier(max_features=1)\n",
    "\n",
    "dt.fit(train_data[0], train_data[1])\n",
    "\n",
    "print(\"train\", accuracy_score(train_data[1], dt.predict(train_data[0])))\n",
    "print(\"test\", accuracy_score(test_data[1], dt.predict(test_data[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('python-chess-lib')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "946d23f443fb1dfa39329254e79dc1d17639e1dd0ec90f8c9f5045c0d1ab1f3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
